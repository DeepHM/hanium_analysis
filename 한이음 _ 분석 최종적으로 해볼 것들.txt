                                  한이음 _ 분석 최종적으로 해볼 것들

1. 뉴스데이터는 .(맞침표)를 기준으로 문장 할당시키고,
   국립원데이터는 kkma.sentences로 문장을 만들어서 문장 리스트 sen을 만든다.

2. sen을 토큰화시켜서 토큰별 빈도수를 구함.

3. 각 문장들 중에서,
    lexrank에서 문장에서 버려지는 부분과
    각 문장들 중 토큰별 빈도수가 1~2인 토큰들이 x% 이상인 문장을 제외시켜서
     exclude_sen 라는 리스트를 만든다.

4. 문장은 토큰들의 집합이다. 그러므로, exclude_sen 에서
    토큰별 빈도수가 1~2인 토큰을 빼버린 문장을 만들어서
    exclude_new_sen 라는 리스트를 만든다.

4.  exclude_new_sen 으로 벡터화를 시킨것으로 군집화를 시켜 군집의 경계지점 문장을 sen에서 찾아서
     군집 n개를 만든다.

5. 4의 결과로 2개의 군집 결과가 필요하다.
   : exclude_new_sen 이 군집화된 군집리스트 : exclude_cluster 라는 리스트
   : exclude_new_sen으로 군집화해서 군집지점을 sen에서 찾은 군집리스트 : sen_cluster

7. exclude_cluster 라는 리스트를 각 군집별로 따로따로 각각씩 해서 lexrank를 돌려,
   군집별 pagerank값 sorting 리스트를 만든다 . ( clus(n)_pr 리스트)

8. clus(n)_pr 리스트에서 상위 x개 or x% 문장을 sen에서 찾아 각 군집별 요약문장을 만든뒤
   최종적으로 군집 요약문장을 통합시켜 최종요약본을 완성한다.

9. 각 군집별 pr값이 하위 1분위수이하에 해당하는 문장들의 리스트를 통합해서
   noise_q1 의 노이즈 리스트를 만든다.

10.  exclude_sen을 만들때 제외된 문장을 찾아 
      noise_exclude 의 리스트를 만든다

11. noise_q1 과 noise_exclude를 합쳐서 최종 노이즈 문장리스트를 만든다. ( fin_noise)